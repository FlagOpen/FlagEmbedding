
<!DOCTYPE html>


<html lang="en" data-content_root="../../" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-X4B1E1Q35K"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-X4B1E1Q35K');
    </script>
    
    <title>BGE Series &#8212; BGE  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=aace3583" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorial/1_Embedding/1.2.1';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="BGE Auto Embedder" href="1.2.2.html" />
    <link rel="prev" title="Intro to Embedding" href="1.1.1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
    
    <img src="../../_static/bge_logo.jpeg" class="logo__image only-light" alt=""/>
    <img src="../../_static/bge_logo.jpeg" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">BGE</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../index.html">
    Home
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../Introduction/index.html">
    Introduction
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../bge/index.html">
    BGE
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../API/index.html">
    API
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button"
                data-bs-toggle="dropdown" aria-expanded="false"
                aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../FAQ/index.html">
    FAQ
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../community/index.html">
    Community
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-external" href="https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d">
    HF Models
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/FlagOpen/FlagEmbedding" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/FlagEmbedding/" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d" title="HF Models" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-cube fa-lg" aria-hidden="true"></i>
            <span class="sr-only">HF Models</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../index.html">
    Home
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../Introduction/index.html">
    Introduction
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../bge/index.html">
    BGE
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../API/index.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../FAQ/index.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d">
    HF Models
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/FlagOpen/FlagEmbedding" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/FlagEmbedding/" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d" title="HF Models" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-cube fa-lg" aria-hidden="true"></i>
            <span class="sr-only">HF Models</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../1_Embedding.html">1. Embedding</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="1.1.1.html">Intro to Embedding</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">BGE Series</a></li>
<li class="toctree-l2"><a class="reference internal" href="1.2.2.html">BGE Auto Embedder</a></li>
<li class="toctree-l2"><a class="reference internal" href="1.2.3.html">BGE Explanation</a></li>
<li class="toctree-l2"><a class="reference internal" href="1.2.4.html">BGE-M3</a></li>
<li class="toctree-l2"><a class="reference internal" href="1.2.5.html">BGE-EN-ICL</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../2_Metrics.html">2. Metrics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../2_Metrics/2.1.html">Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2_Metrics/2.2.html">Evaluation Metrics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../3_Indexing.html">3. Indexing</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_Indexing/3.1.1.html">Indexing Using Faiss</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_Indexing/3.1.2.html">Faiss GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_Indexing/3.1.3.html">Faiss Indexes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_Indexing/3.1.4.html">Faiss Quantizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_Indexing/3.1.5.html">Choosing Index</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../4_Evaluation.html">4. Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../4_Evaluation/4.1.1.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_Evaluation/4.2.1.html">MTEB</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_Evaluation/4.2.2.html">MTEB Leaderboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_Evaluation/4.2.3.html">C-MTEB</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_Evaluation/4.3.1.html">Evaluation Using Sentence Transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_Evaluation/4.4.1.html">Evaluate on BEIR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_Evaluation/4.5.1.html">Evaluate on MIRACL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_Evaluation/4.5.2.html">Evaluate on MLDR</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../5_Reranking.html">5. Reranking</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../5_Reranking/5.1.html">Reranker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5_Reranking/5.2.html">BGE Reranker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5_Reranking/5.3.html">Evaluate Reranker</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../6_RAG.html">6. RAG</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../6_RAG/6.1.html">Simple RAG From Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6_RAG/6.2.html">RAG with LangChain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6_RAG/6.3.html">RAG with LlamaIndex</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../7_Finetuning.html">7. Finetuning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../7_Finetuning/7.1.1.html">Data Preparation for Fine-tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7_Finetuning/7.1.2.html">Fine-tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7_Finetuning/7.1.3.html">Evaluate the Fine-tuned Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7_Finetuning/7.2.1.html">Hard Negatives</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Tutorials</a></li>
    
    
    <li class="breadcrumb-item"><a href="../1_Embedding.html" class="nav-link">1. Embedding</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">BGE Series</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="bge-series">
<h1>BGE Series<a class="headerlink" href="#bge-series" title="Link to this heading">#</a></h1>
<p>In this Part, we will walk through the BGE series and introduce how to use the BGE embedding models.</p>
<section id="baai-general-embedding">
<h2>1. BAAI General Embedding<a class="headerlink" href="#baai-general-embedding" title="Link to this heading">#</a></h2>
<p>BGE stands for BAAI General Embedding, it’s a series of embeddings models developed and published by Beijing Academy of Artificial Intelligence (BAAI).</p>
<p>A full support of APIs and related usages of BGE is maintained in <a class="reference external" href="https://github.com/FlagOpen/FlagEmbedding">FlagEmbedding</a> on GitHub.</p>
<p>Run the following cell to install FlagEmbedding in your environment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%capture</span>
<span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">FlagEmbedding</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span> 
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TRANSFORMERS_NO_ADVISORY_WARNINGS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;true&#39;</span>
<span class="c1"># single GPU is better for small tasks</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_VISIBLE_DEVICES&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0&#39;</span>
</pre></div>
</div>
</div>
</div>
<p>The collection of BGE models can be found in <a class="reference external" href="https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d">Huggingface collection</a>.</p>
</section>
<section id="bge-series-models">
<h2>2. BGE Series Models<a class="headerlink" href="#bge-series-models" title="Link to this heading">#</a></h2>
<section id="bge">
<h3>2.1 BGE<a class="headerlink" href="#bge" title="Link to this heading">#</a></h3>
<p>The very first version of BGE has 6 models, with ‘large’, ‘base’, and ‘small’ for English and Chinese.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-center"><p>Language</p></th>
<th class="head text-center"><p>Parameters</p></th>
<th class="head text-center"><p>Model Size</p></th>
<th class="head text-center"><p>Description</p></th>
<th class="head text-center"><p>Base Model</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-large-en">BAAI/bge-large-en</a></p></td>
<td class="text-center"><p>English</p></td>
<td class="text-center"><p>500M</p></td>
<td class="text-center"><p>1.34 GB</p></td>
<td class="text-center"><p>Embedding Model which map text into vector</p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-base-en">BAAI/bge-base-en</a></p></td>
<td class="text-center"><p>English</p></td>
<td class="text-center"><p>109M</p></td>
<td class="text-center"><p>438 MB</p></td>
<td class="text-center"><p>a base-scale model but with similar ability to <code class="docutils literal notranslate"><span class="pre">bge-large-en</span></code></p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-small-en">BAAI/bge-small-en</a></p></td>
<td class="text-center"><p>English</p></td>
<td class="text-center"><p>33.4M</p></td>
<td class="text-center"><p>133 MB</p></td>
<td class="text-center"><p>a small-scale model but with competitive performance</p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-large-zh">BAAI/bge-large-zh</a></p></td>
<td class="text-center"><p>Chinese</p></td>
<td class="text-center"><p>326M</p></td>
<td class="text-center"><p>1.3 GB</p></td>
<td class="text-center"><p>Embedding Model which map text into vector</p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-base-zh">BAAI/bge-base-zh</a></p></td>
<td class="text-center"><p>Chinese</p></td>
<td class="text-center"><p>102M</p></td>
<td class="text-center"><p>409 MB</p></td>
<td class="text-center"><p>a base-scale model but with similar ability to <code class="docutils literal notranslate"><span class="pre">bge-large-zh</span></code></p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-small-zh">BAAI/bge-small-zh</a></p></td>
<td class="text-center"><p>Chinese</p></td>
<td class="text-center"><p>24M</p></td>
<td class="text-center"><p>95.8 MB</p></td>
<td class="text-center"><p>a small-scale model but with competitive performance</p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
</tbody>
</table>
</div>
<p>For inference, simply import FlagModel from FlagEmbedding and initialize the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">FlagEmbedding</span> <span class="kn">import</span> <span class="n">FlagModel</span>

<span class="c1"># Load BGE model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FlagModel</span><span class="p">(</span>
    <span class="s1">&#39;BAAI/bge-base-en&#39;</span><span class="p">,</span>
    <span class="n">query_instruction_for_retrieval</span><span class="o">=</span><span class="s2">&quot;Represent this sentence for searching relevant passages:&quot;</span><span class="p">,</span>
    <span class="n">query_instruction_format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}{}</span><span class="s1">&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;query 1&quot;</span><span class="p">,</span> <span class="s2">&quot;query 2&quot;</span><span class="p">]</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;passage 1&quot;</span><span class="p">,</span> <span class="s2">&quot;passage 2&quot;</span><span class="p">]</span>

<span class="c1"># encode the queries and corpus</span>
<span class="n">q_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_queries</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
<span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># compute the similarity scores</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">q_embeddings</span> <span class="o">@</span> <span class="n">p_embeddings</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.84864    0.7946737 ]
 [0.760097   0.85449743]]
</pre></div>
</div>
</div>
</div>
<p>For general encoding, use either <code class="docutils literal notranslate"><span class="pre">encode()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FlagModel</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>or <code class="docutils literal notranslate"><span class="pre">encode_corpus()</span></code> that directly calls <code class="docutils literal notranslate"><span class="pre">encode()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FlagModel</span><span class="o">.</span><span class="n">encode_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The <em>encode_queries()</em> function concatenate the <code class="docutils literal notranslate"><span class="pre">query_instruction_for_retrieval</span></code> with each of the input query to form the new sentences and then feed them to <code class="docutils literal notranslate"><span class="pre">encode()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FlagModel</span><span class="o">.</span><span class="n">encode_queries</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="bge-v1-5">
<h3>2.2 BGE v1.5<a class="headerlink" href="#bge-v1-5" title="Link to this heading">#</a></h3>
<p>BGE 1.5 alleviate the issue of the similarity distribution, and enhance retrieval ability without instruction.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-center"><p>Language</p></th>
<th class="head text-center"><p>Parameters</p></th>
<th class="head text-center"><p>Model Size</p></th>
<th class="head text-center"><p>Description</p></th>
<th class="head text-center"><p>Base Model</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-large-en-v1.5">BAAI/bge-large-en-v1.5</a></p></td>
<td class="text-center"><p>English</p></td>
<td class="text-center"><p>335M</p></td>
<td class="text-center"><p>1.34 GB</p></td>
<td class="text-center"><p>version 1.5 with more reasonable similarity distribution</p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-base-en-v1.5">BAAI/bge-base-en-v1.5</a></p></td>
<td class="text-center"><p>English</p></td>
<td class="text-center"><p>109M</p></td>
<td class="text-center"><p>438 MB</p></td>
<td class="text-center"><p>version 1.5 with more reasonable similarity distribution</p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-small-en-v1.5">BAAI/bge-small-en-v1.5</a></p></td>
<td class="text-center"><p>English</p></td>
<td class="text-center"><p>33.4M</p></td>
<td class="text-center"><p>133 MB</p></td>
<td class="text-center"><p>version 1.5 with more reasonable similarity distribution</p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-large-zh-v1.5">BAAI/bge-large-zh-v1.5</a></p></td>
<td class="text-center"><p>Chinese</p></td>
<td class="text-center"><p>326M</p></td>
<td class="text-center"><p>1.3 GB</p></td>
<td class="text-center"><p>version 1.5 with more reasonable similarity distribution</p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-base-zh-v1.5">BAAI/bge-base-zh-v1.5</a></p></td>
<td class="text-center"><p>Chinese</p></td>
<td class="text-center"><p>102M</p></td>
<td class="text-center"><p>409 MB</p></td>
<td class="text-center"><p>version 1.5 with more reasonable similarity distribution</p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-small-zh-v1.5">BAAI/bge-small-zh-v1.5</a></p></td>
<td class="text-center"><p>Chinese</p></td>
<td class="text-center"><p>24M</p></td>
<td class="text-center"><p>95.8 MB</p></td>
<td class="text-center"><p>version 1.5 with more reasonable similarity distribution</p></td>
<td class="text-center"><p>BERT</p></td>
</tr>
</tbody>
</table>
</div>
<p>You can use BGE 1.5 models exactly same to BGE v1 models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">FlagModel</span><span class="p">(</span>
    <span class="s1">&#39;BAAI/bge-base-en-v1.5&#39;</span><span class="p">,</span>
    <span class="n">query_instruction_for_retrieval</span><span class="o">=</span><span class="s2">&quot;Represent this sentence for searching relevant passages:&quot;</span><span class="p">,</span>
    <span class="n">query_instruction_format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}{}</span><span class="s1">&#39;</span>
<span class="p">)</span>

<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;query 1&quot;</span><span class="p">,</span> <span class="s2">&quot;query 2&quot;</span><span class="p">]</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;passage 1&quot;</span><span class="p">,</span> <span class="s2">&quot;passage 2&quot;</span><span class="p">]</span>

<span class="c1"># encode the queries and corpus</span>
<span class="n">q_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_queries</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
<span class="n">p_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># compute the similarity scores</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">q_embeddings</span> <span class="o">@</span> <span class="n">p_embeddings</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>pre tokenize: 100%|██████████| 1/1 [00:00&lt;00:00, 2252.58it/s]
pre tokenize: 100%|██████████| 1/1 [00:00&lt;00:00, 3575.71it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.76   0.6714]
 [0.6177 0.7603]]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="bge-m3">
<h3>2.3 BGE M3<a class="headerlink" href="#bge-m3" title="Link to this heading">#</a></h3>
<p>BGE-M3 is the new version of BGE models that is distinguished for its versatility in:</p>
<ul class="simple">
<li><p>Multi-Functionality: Simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.</p></li>
<li><p>Multi-Linguality: Supports more than 100 working languages.</p></li>
<li><p>Multi-Granularity: Can proces inputs with different granularityies, spanning from short sentences to long documents of up to 8192 tokens.</p></li>
</ul>
<p>For more details, feel free to check out the <a class="reference external" href="https://arxiv.org/pdf/2402.03216">paper</a>.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-center"><p>Language</p></th>
<th class="head text-center"><p>Parameters</p></th>
<th class="head text-center"><p>Model Size</p></th>
<th class="head text-center"><p>Description</p></th>
<th class="head text-center"><p>Base Model</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-m3">BAAI/bge-m3</a></p></td>
<td class="text-center"><p>Multilingual</p></td>
<td class="text-center"><p>568M</p></td>
<td class="text-center"><p>2.27 GB</p></td>
<td class="text-center"><p>Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens)</p></td>
<td class="text-center"><p>XLM-RoBERTa</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">FlagEmbedding</span> <span class="kn">import</span> <span class="n">BGEM3FlagModel</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BGEM3FlagModel</span><span class="p">(</span><span class="s1">&#39;BAAI/bge-m3&#39;</span><span class="p">,</span> <span class="n">use_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;What is BGE M3?&quot;</span><span class="p">,</span> <span class="s2">&quot;Defination of BM25&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fetching 30 files: 100%|██████████| 30/30 [00:00&lt;00:00, 194180.74it/s]
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BGEM3FlagModel</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">sentences</span><span class="p">,</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
    <span class="n">max_length</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> 
    <span class="n">return_dense</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">return_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">return_colbert_vecs</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
<p>It returns a dictionary like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;dense_vecs&#39;</span><span class="p">:</span>       <span class="c1"># array of dense embeddings of inputs if return_dense=True, otherwise None,</span>
    <span class="s1">&#39;lexical_weights&#39;</span><span class="p">:</span>  <span class="c1"># array of dictionaries with keys and values are ids of tokens and their corresponding weights if return_sparse=True, otherwise None,</span>
    <span class="s1">&#39;colbert_vecs&#39;</span><span class="p">:</span>     <span class="c1"># array of multi-vector embeddings of inputs if return_cobert_vecs=True, otherwise None,&#39;</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># If you don&#39;t need such a long length of 8192 input tokens, you can set max_length to a smaller value to speed up encoding.</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">sentences</span><span class="p">,</span> 
    <span class="n">max_length</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">return_dense</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">return_sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">return_colbert_vecs</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>pre tokenize: 100%|██████████| 1/1 [00:00&lt;00:00, 1148.18it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dense embedding:</span><span class="se">\n</span><span class="si">{</span><span class="n">embeddings</span><span class="p">[</span><span class="s1">&#39;dense_vecs&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sparse embedding:</span><span class="se">\n</span><span class="si">{</span><span class="n">embeddings</span><span class="p">[</span><span class="s1">&#39;lexical_weights&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;multi-vector:</span><span class="se">\n</span><span class="si">{</span><span class="n">embeddings</span><span class="p">[</span><span class="s1">&#39;colbert_vecs&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dense embedding:
[[-0.03412  -0.04706  -0.00087  ...  0.04822   0.007614 -0.02957 ]
 [-0.01035  -0.04483  -0.02434  ... -0.008224  0.01497   0.011055]]
sparse embedding:
[defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;4865&#39;: np.float16(0.0836), &#39;83&#39;: np.float16(0.0814), &#39;335&#39;: np.float16(0.1296), &#39;11679&#39;: np.float16(0.2517), &#39;276&#39;: np.float16(0.1699), &#39;363&#39;: np.float16(0.2695), &#39;32&#39;: np.float16(0.04077)}), defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;262&#39;: np.float16(0.05014), &#39;5983&#39;: np.float16(0.1367), &#39;2320&#39;: np.float16(0.04517), &#39;111&#39;: np.float16(0.0634), &#39;90017&#39;: np.float16(0.2517), &#39;2588&#39;: np.float16(0.3333)})]
multi-vector:
[array([[-8.68966337e-03, -4.89266850e-02, -3.03634931e-03, ...,
        -2.21243706e-02,  5.72856329e-02,  1.28355855e-02],
       [-8.92937183e-03, -4.67235669e-02, -9.52814799e-03, ...,
        -3.14785317e-02,  5.39088845e-02,  6.96671568e-03],
       [ 1.84195358e-02, -4.22310382e-02,  8.55499704e-04, ...,
        -1.97946690e-02,  3.84313315e-02,  7.71250250e-03],
       ...,
       [-2.55824160e-02, -1.65533274e-02, -4.21357416e-02, ...,
        -4.50234264e-02,  4.41286489e-02, -1.00052059e-02],
       [ 5.90990965e-07, -5.53734899e-02,  8.51499755e-03, ...,
        -2.29209941e-02,  6.04418293e-02,  9.39912070e-03],
       [ 2.57394509e-03, -2.92690992e-02, -1.89342294e-02, ...,
        -8.04431178e-03,  3.28964666e-02,  4.38723788e-02]], dtype=float32), array([[ 0.01724418,  0.03835401, -0.02309308, ...,  0.00141706,
         0.02995041, -0.05990082],
       [ 0.00996325,  0.03922409, -0.03849588, ...,  0.00591671,
         0.02722516, -0.06510868],
       [ 0.01781915,  0.03925728, -0.01710397, ...,  0.00801776,
         0.03987768, -0.05070014],
       ...,
       [ 0.05478653,  0.00755799,  0.00328444, ..., -0.01648209,
         0.02405782,  0.00363262],
       [ 0.00936953,  0.05028074, -0.02388872, ...,  0.02567679,
         0.00791224, -0.03257877],
       [ 0.01803976,  0.0133922 ,  0.00019365, ...,  0.0184015 ,
         0.01373822,  0.00315539]], dtype=float32)]
</pre></div>
</div>
</div>
</div>
</section>
<section id="bge-multilingual-gemma2">
<h3>2.4 BGE Multilingual Gemma2<a class="headerlink" href="#bge-multilingual-gemma2" title="Link to this heading">#</a></h3>
<p>BGE Multilingual Gemma2 is a LLM-based Multi-Lingual embedding model.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-center"><p>Language</p></th>
<th class="head text-center"><p>Parameters</p></th>
<th class="head text-center"><p>Model Size</p></th>
<th class="head text-center"><p>Description</p></th>
<th class="head text-center"><p>Base Model</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-multilingual-gemma2">BAAI/bge-multilingual-gemma2</a></p></td>
<td class="text-center"><p>Multilingual</p></td>
<td class="text-center"><p>9.24B</p></td>
<td class="text-center"><p>37 GB</p></td>
<td class="text-center"><p>LLM-based multilingual embedding model with SOTA results on multilingual benchmarks</p></td>
<td class="text-center"><p>Gemma2-9B</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">FlagEmbedding</span> <span class="kn">import</span> <span class="n">FlagLLMModel</span>

<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;how much protein should a female eat&quot;</span><span class="p">,</span> <span class="s2">&quot;summit define&quot;</span><span class="p">]</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;As a general guideline, the CDC&#39;s average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you&#39;ll need to increase that if you&#39;re expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.&quot;</span>
<span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">FlagLLMModel</span><span class="p">(</span><span class="s1">&#39;BAAI/bge-multilingual-gemma2&#39;</span><span class="p">,</span> 
                     <span class="n">query_instruction_for_retrieval</span><span class="o">=</span><span class="s2">&quot;Given a web search query, retrieve relevant passages that answer the query.&quot;</span><span class="p">,</span>
                     <span class="n">use_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Setting use_fp16 to True speeds up computation with a slight performance degradation</span>

<span class="n">embeddings_1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_queries</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
<span class="n">embeddings_2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_corpus</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">embeddings_1</span> <span class="o">@</span> <span class="n">embeddings_2</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">similarity</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading checkpoint shards: 100%|██████████| 4/4 [00:00&lt;00:00,  6.34it/s]
pre tokenize: 100%|██████████| 1/1 [00:00&lt;00:00, 816.49it/s]
pre tokenize: 100%|██████████| 1/1 [00:00&lt;00:00, 718.33it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.559     0.01685  ]
 [0.0008683 0.5015   ]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="bge-icl">
<h3>2.4 BGE ICL<a class="headerlink" href="#bge-icl" title="Link to this heading">#</a></h3>
<p>BGE ICL stands for in-context learning. By providing few-shot examples in the query, it can significantly enhance the model’s ability to handle new tasks.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-center"><p>Language</p></th>
<th class="head text-center"><p>Parameters</p></th>
<th class="head text-center"><p>Model Size</p></th>
<th class="head text-center"><p>Description</p></th>
<th class="head text-center"><p>Base Model</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/BAAI/bge-en-icl">BAAI/bge-en-icl</a></p></td>
<td class="text-center"><p>English</p></td>
<td class="text-center"><p>7.11B</p></td>
<td class="text-center"><p>28.5 GB</p></td>
<td class="text-center"><p>LLM-based English embedding model with excellent in-context learning ability.</p></td>
<td class="text-center"><p>Mistral-7B</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;As a general guideline, the CDC&#39;s average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you&#39;ll need to increase that if you&#39;re expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.&quot;</span>
<span class="p">]</span>

<span class="n">examples</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s1">&#39;instruct&#39;</span><span class="p">:</span> <span class="s1">&#39;Given a web search query, retrieve relevant passages that answer the query.&#39;</span><span class="p">,</span>
        <span class="s1">&#39;query&#39;</span><span class="p">:</span> <span class="s1">&#39;what is a virtual interface&#39;</span><span class="p">,</span>
        <span class="s1">&#39;response&#39;</span><span class="p">:</span> <span class="s2">&quot;A virtual interface is a software-defined abstraction that mimics the behavior and characteristics of a physical network interface. It allows multiple logical network connections to share the same physical network interface, enabling efficient utilization of network resources. Virtual interfaces are commonly used in virtualization technologies such as virtual machines and containers to provide network connectivity without requiring dedicated hardware. They facilitate flexible network configurations and help in isolating network traffic for security and management purposes.&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s1">&#39;instruct&#39;</span><span class="p">:</span> <span class="s1">&#39;Given a web search query, retrieve relevant passages that answer the query.&#39;</span><span class="p">,</span>
        <span class="s1">&#39;query&#39;</span><span class="p">:</span> <span class="s1">&#39;causes of back pain in female for a week&#39;</span><span class="p">,</span>
        <span class="s1">&#39;response&#39;</span><span class="p">:</span> <span class="s2">&quot;Back pain in females lasting a week can stem from various factors. Common causes include muscle strain due to lifting heavy objects or improper posture, spinal issues like herniated discs or osteoporosis, menstrual cramps causing referred pain, urinary tract infections, or pelvic inflammatory disease. Pregnancy-related changes can also contribute. Stress and lack of physical activity may exacerbate symptoms. Proper diagnosis by a healthcare professional is crucial for effective treatment and management.&quot;</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;how much protein should a female eat&quot;</span><span class="p">,</span> <span class="s2">&quot;summit define&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">FlagEmbedding</span> <span class="kn">import</span> <span class="n">FlagICLModel</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">FlagICLModel</span><span class="p">(</span><span class="s1">&#39;BAAI/bge-en-icl&#39;</span><span class="p">,</span> 
                     <span class="n">examples_for_task</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>  <span class="c1"># set `examples_for_task=None` to use model without examples</span>
                    <span class="c1">#  examples_instruction_format=&quot;&lt;instruct&gt;{}\n&lt;query&gt;{}\n&lt;response&gt;{}&quot; # specify the format to use examples_for_task</span>
                     <span class="p">)</span>

<span class="n">embeddings_1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_queries</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
<span class="n">embeddings_2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_corpus</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">embeddings_1</span> <span class="o">@</span> <span class="n">embeddings_2</span><span class="o">.</span><span class="n">T</span>

<span class="nb">print</span><span class="p">(</span><span class="n">similarity</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading checkpoint shards: 100%|██████████| 3/3 [00:00&lt;00:00,  6.55it/s]
pre tokenize: 100%|██████████| 1/1 [00:00&lt;00:00, 366.09it/s]
pre tokenize: 100%|██████████| 1/1 [00:00&lt;00:00, 623.69it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.6064 0.3018]
 [0.257  0.537 ]]
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="1.1.1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Intro to Embedding</p>
      </div>
    </a>
    <a class="right-next"
       href="1.2.2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">BGE Auto Embedder</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baai-general-embedding">1. BAAI General Embedding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bge-series-models">2. BGE Series Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bge">2.1 BGE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bge-v1-5">2.2 BGE v1.5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bge-m3">2.3 BGE M3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bge-multilingual-gemma2">2.4 BGE Multilingual Gemma2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bge-icl">2.4 BGE ICL</a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/tutorial/1_Embedding/1.2.1.ipynb.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, BAAI.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>