{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForSequenceClassification, PreTrainedModel, TrainingArguments, AutoTokenizer\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/experiments/llm/fcbank/reranker_group30_batch2_v100\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/experiments/llm/fcbank/reranker_group30_batch2_v100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 假设 model 是已经加载的模型\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(...)\n",
    "\n",
    "# 假设 group_size 是你的模型处理的样本数量\n",
    "group_size = 15  # 根据实际情况设置\n",
    "batch_size = 2\n",
    "\n",
    "# 假设你已经有一个文本序列和对应的标签\n",
    "text = \"这是一个示例文本。\"\n",
    "label = 1  # 假设标签是1\n",
    "\n",
    "# 使用模型的分词器对文本进行编码\n",
    "encoded_input = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# 调整编码后的输入以匹配 group_size\n",
    "input_ids = encoded_input['input_ids'].repeat_interleave(batch_size*group_size, dim=0).cuda()\n",
    "attention_mask = encoded_input['attention_mask'].repeat_interleave(batch_size*group_size, dim=0).cuda()\n",
    "\n",
    "print(type(input_ids))\n",
    "\n",
    "# 创建 batch 字典\n",
    "batch = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask,\n",
    "}\n",
    "\n",
    "labels = torch.tensor([label]*batch_size, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(input_ids, attention_mask, model=model, tokenizer=tokenizer):\n",
    "    hidden_state = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True).hidden_states[-1].cpu()\n",
    "    attention_mask = attention_mask.cpu()\n",
    "    seq_lengths = attention_mask.sum(dim=1)\n",
    "    embeddings = []\n",
    "    for seq_len, seq_emb in zip(seq_lengths, hidden_state):\n",
    "        valid_emb = seq_emb[:seq_len]\n",
    "        embeddings.append(torch.mean(valid_emb, dim=0))\n",
    "\n",
    "    embedding = torch.stack(embeddings)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(batch):\n",
    "    cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "    embeddings = get_embedding(**batch)\n",
    "    loss = batchloss(embeddings)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402],\n",
      "        [-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402],\n",
      "        [-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402],\n",
      "        ...,\n",
      "        [-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402],\n",
      "        [-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402],\n",
      "        [-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402]],\n",
      "       dtype=torch.float16, grad_fn=<StackBackward0>)\n",
      "torch.Size([30, 768])\n"
     ]
    }
   ],
   "source": [
    "results = get_embedding(**batch)\n",
    "print(results)\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "pred = results.view(batch_size, group_size, -1)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def infoNCELoss(anchor, positive, negatives, temperature=1):\n",
    "    # 计算所有样本的相似度\n",
    "    pos_similarity = F.cosine_similarity(anchor, positive, dim=-1)\n",
    "    # 将anchor重复到与负样本相同数量的维度，以便计算\n",
    "    neg_similarity = F.cosine_similarity(anchor, negatives, dim=-1)\n",
    "    # 合并正样本和负样本的相似度\n",
    "    all_similarity = torch.cat([pos_similarity, neg_similarity])\n",
    "    # 应用温度缩放\n",
    "    all_similarity /= temperature\n",
    "    # 计算InfoNCE损失\n",
    "    loss = - torch.log(torch.exp(pos_similarity)/torch.sum(torch.exp(all_similarity)))\n",
    "    return loss.mean()\n",
    "\n",
    "def batchloss(embeddings):\n",
    "    # 遍历每个batch计算损失\n",
    "    losses = []\n",
    "    for i in range(embeddings.size(0)):\n",
    "        # anchor embeddings\n",
    "        anchor = embeddings[i, 0].unsqueeze(0)  # [1, 768]\n",
    "        # positive embeddings\n",
    "        positive = embeddings[i, 1].unsqueeze(0)  # [1, 768]\n",
    "        # 除了anchor和positive之外的所有embeddings作为负样本\n",
    "        negatives = embeddings[i, 2:]  # [13, 768]\n",
    "        # 计算当前batch的InfoNCE损失\n",
    "        loss = infoNCELoss(anchor, positive, negatives)\n",
    "        losses.append(loss)\n",
    "    # 计算整个batch的平均损失\n",
    "    batch_loss = torch.mean(torch.stack(losses))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6431)\n"
     ]
    }
   ],
   "source": [
    "# 假设 embeddings 是一个形状为 [batch, group_size, embedding_len] 的张量\n",
    "embeddings = torch.randn(2, 15, 768)  # 示例数据\n",
    "print(batchloss(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去掉模型的分类头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaModel(\n",
      "  (embeddings): XLMRobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): XLMRobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x XLMRobertaLayer(\n",
      "        (attention): XLMRobertaAttention(\n",
      "          (self): XLMRobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): XLMRobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): XLMRobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): XLMRobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): XLMRobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification, AutoModel, AutoTokenizer\n",
    "import torch\n",
    "model = AutoModel.from_pretrained('/mnt/bn/data-tns-live-llm/leon/experiments/llm/fcbank/xlmr/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/', torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/experiments/llm/fcbank/xlmr/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/\")\n",
    "# print(type(model.modules()))\n",
    "print(model)\n",
    "# print(model.roberta)\n",
    "# print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaForSequenceClassification(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "del model.classifier\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看看 Data 构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"hello\"\n",
    "pos = \"hello\"\n",
    "negs = [\"hello\",\"hello\"]\n",
    "\n",
    "batch_data = tokenizer([query]+[pos]+negs, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看看开源的sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb32cfe5d0004229bc9805b89f8aada3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00264f2089924461a5b29c6f08d4a0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73805243674d41128b6d79ff8b9bcd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/439k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1216dad3d90b4a799c4c306d1ed80da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b25020975fa46e4b0692e9b132c95ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa40cf6b31b14b71b6cd6d7a0484a27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 8*GPUs----------\n",
      "n192-024-092:113099:113099 [0] NCCL INFO cudaDriverVersion 12010\n",
      "n192-024-092:113099:113099 [0] NCCL INFO NCCL_SOCKET_FAMILY set by environment to AF_INET6\n",
      "n192-024-092:113099:113099 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\n",
      "n192-024-092:113099:113099 [0] NCCL INFO Bootstrap : Using eth0:fdbd:dc61:7:34::92<0>\n",
      "n192-024-092:113099:113099 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\n",
      "NCCL version 2.20.5+cuda12.4\n",
      "n192-024-092:113099:271766 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.\n",
      "n192-024-092:113099:271766 [0] NCCL INFO NCCL_SOCKET_FAMILY set by environment to AF_INET6\n",
      "n192-024-092:113099:271766 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\n",
      "n192-024-092:113099:271766 [0] NCCL INFO NCCL_IB_HCA set to mlx5_2:1\n",
      "n192-024-092:113099:271766 [0] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [RO]; OOB eth0:fdbd:dc61:7:34::92<0>\n",
      "n192-024-092:113099:271766 [0] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-092:113099:271766 [0] NCCL INFO Using network IB\n",
      "n192-024-092:113099:271767 [1] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-092:113099:271767 [1] NCCL INFO Using network IB\n",
      "n192-024-092:113099:271766 [0] NCCL INFO comm 0xb205b610 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xa0b4958593d819c3 - Init START\n",
      "n192-024-092:113099:271767 [1] NCCL INFO comm 0xb205e980 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 1c000 commId 0xa0b4958593d819c3 - Init START\n",
      "n192-024-092:113099:271767 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\n",
      "n192-024-092:113099:271766 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "n192-024-092:113099:271766 [0] NCCL INFO comm 0xb205b610 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n",
      "n192-024-092:113099:271766 [0] NCCL INFO Channel 00/02 :    0   1\n",
      "n192-024-092:113099:271767 [1] NCCL INFO comm 0xb205e980 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n",
      "n192-024-092:113099:271766 [0] NCCL INFO Channel 01/02 :    0   1\n",
      "n192-024-092:113099:271766 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\n",
      "n192-024-092:113099:271766 [0] NCCL INFO P2P Chunksize set to 524288\n",
      "n192-024-092:113099:271767 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0\n",
      "n192-024-092:113099:271767 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "n192-024-092:113099:271766 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/direct pointer\n",
      "n192-024-092:113099:271767 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/direct pointer\n",
      "n192-024-092:113099:271766 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/direct pointer\n",
      "n192-024-092:113099:271767 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/direct pointer\n",
      "n192-024-092:113099:271766 [0] NCCL INFO Connected all rings\n",
      "n192-024-092:113099:271766 [0] NCCL INFO Connected all trees\n",
      "n192-024-092:113099:271767 [1] NCCL INFO Connected all rings\n",
      "n192-024-092:113099:271767 [1] NCCL INFO Connected all trees\n",
      "n192-024-092:113099:271767 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "n192-024-092:113099:271767 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "n192-024-092:113099:271766 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "n192-024-092:113099:271766 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "n192-024-092:113099:271767 [1] NCCL INFO comm 0xb205e980 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 1c000 commId 0xa0b4958593d819c3 - Init COMPLETE\n",
      "n192-024-092:113099:271766 [0] NCCL INFO comm 0xb205b610 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xa0b4958593d819c3 - Init COMPLETE\n",
      "[[0.855  0.852 ]\n",
      " [0.874  0.8555]]\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Using network IB\n",
      "n192-024-092:113099:271788 [0] NCCL INFO comm 0xb80a5d70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xdd4574414117f6ad - Init START\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "n192-024-092:113099:271788 [0] NCCL INFO comm 0xb80a5d70 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 00/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 01/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 02/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 03/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 04/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 05/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 06/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 07/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 08/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 09/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 10/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 11/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 12/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 13/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 14/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 15/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 16/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 17/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 18/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 19/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 20/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 21/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 22/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 23/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 24/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 25/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 26/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 27/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 28/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 29/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 30/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Channel 31/32 :    0\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "n192-024-092:113099:271788 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Connected all rings\n",
      "n192-024-092:113099:271788 [0] NCCL INFO Connected all trees\n",
      "n192-024-092:113099:271788 [0] NCCL INFO 32 coll channels, 0 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "n192-024-092:113099:271788 [0] NCCL INFO comm 0xb80a5d70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xdd4574414117f6ad - Init COMPLETE\n",
      "[[0.652  0.4424]]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "sentences_1 = [\"样例数据-1\", \"样例数据-2\"]\n",
    "sentences_2 = [\"样例数据-3\", \"样例数据-4\"]\n",
    "model = FlagModel('BAAI/bge-large-zh-v1.5', \n",
    "                  query_instruction_for_retrieval=\"为这个句子生成表示以用于检索相关文章：\",\n",
    "                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "embeddings_1 = model.encode(sentences_1)\n",
    "embeddings_2 = model.encode(sentences_2)\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity) \n",
    "\n",
    "# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n",
    "# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\n",
    "queries = ['When was quantum field theory developed?']\n",
    "passages = [\"Quantum field theory naturally began with the study of electromagnetic interactions, as the electromagnetic field was the only known classical field as of the 1920s.[8]:1\", \"Cumrun Vafa is a string theorist. His research is focused on the nature of quantum gravity and the relation between geometry and quantum field theories. He is known in the string theory community for his co-discovery, with Strominger, that the Bekenstein-Hawking entropy of a black hole can be accounted for by solitonic states of superstring theory, and for expounding the relation between geometry and field theories that arise through string dualities (culminating in the Gopakumar\\u2013Vafa conjecture). This topic has been known as \\\"geometric engineering of quantum field theories\\\". In 1997, he developed F-theory.\"]\n",
    "q_embeddings = model.encode_queries(queries)\n",
    "p_embeddings = model.encode(passages)\n",
    "scores = q_embeddings @ p_embeddings.T\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
