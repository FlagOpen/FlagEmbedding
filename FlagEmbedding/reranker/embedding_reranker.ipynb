{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForSequenceClassification, PreTrainedModel, TrainingArguments, AutoTokenizer\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/experiments/llm/fcbank/reranker_group30_batch2_v100\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/experiments/llm/fcbank/reranker_group30_batch2_v100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 假设 model 是已经加载的模型\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(...)\n",
    "\n",
    "# 假设 group_size 是你的模型处理的样本数量\n",
    "group_size = 15  # 根据实际情况设置\n",
    "batch_size = 2\n",
    "\n",
    "# 假设你已经有一个文本序列和对应的标签\n",
    "text = \"这是一个示例文本。\"\n",
    "label = 1  # 假设标签是1\n",
    "\n",
    "# 使用模型的分词器对文本进行编码\n",
    "encoded_input = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# 调整编码后的输入以匹配 group_size\n",
    "input_ids = encoded_input['input_ids'].repeat_interleave(batch_size*group_size, dim=0).cuda()\n",
    "attention_mask = encoded_input['attention_mask'].repeat_interleave(batch_size*group_size, dim=0).cuda()\n",
    "\n",
    "print(type(input_ids))\n",
    "\n",
    "# 创建 batch 字典\n",
    "batch = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask,\n",
    "}\n",
    "\n",
    "labels = torch.tensor([label]*batch_size, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(input_ids, attention_mask, model=model, tokenizer=tokenizer):\n",
    "    hidden_state = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True).hidden_states[-1].cpu()\n",
    "    attention_mask = attention_mask.cpu()\n",
    "    seq_lengths = attention_mask.sum(dim=1)\n",
    "    embeddings = []\n",
    "    for seq_len, seq_emb in zip(seq_lengths, hidden_state):\n",
    "        valid_emb = seq_emb[:seq_len]\n",
    "        embeddings.append(torch.mean(valid_emb, dim=0))\n",
    "\n",
    "    embedding = torch.stack(embeddings)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(batch):\n",
    "    cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "    embeddings = get_embedding(**batch)\n",
    "    loss = batchloss(embeddings)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402],\n",
      "        [-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402],\n",
      "        [-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402],\n",
      "        ...,\n",
      "        [-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402],\n",
      "        [-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402],\n",
      "        [-0.2715, -0.0060, -0.6865,  ..., -0.9995, -0.5493,  0.4402]],\n",
      "       dtype=torch.float16, grad_fn=<StackBackward0>)\n",
      "torch.Size([30, 768])\n"
     ]
    }
   ],
   "source": [
    "results = get_embedding(**batch)\n",
    "print(results)\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "pred = results.view(batch_size, group_size, -1)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def infoNCELoss(anchor, positive, negatives, temperature=1):\n",
    "    # 计算所有样本的相似度\n",
    "    pos_similarity = F.cosine_similarity(anchor, positive, dim=-1)\n",
    "    # 将anchor重复到与负样本相同数量的维度，以便计算\n",
    "    neg_similarity = F.cosine_similarity(anchor, negatives, dim=-1)\n",
    "    # 合并正样本和负样本的相似度\n",
    "    all_similarity = torch.cat([pos_similarity, neg_similarity])\n",
    "    # 应用温度缩放\n",
    "    all_similarity /= temperature\n",
    "    # 计算InfoNCE损失\n",
    "    loss = - torch.log(torch.exp(pos_similarity)/torch.sum(torch.exp(all_similarity)))\n",
    "    return loss.mean()\n",
    "\n",
    "def batchloss(embeddings):\n",
    "    # 遍历每个batch计算损失\n",
    "    losses = []\n",
    "    for i in range(embeddings.size(0)):\n",
    "        # anchor embeddings\n",
    "        anchor = embeddings[i, 0].unsqueeze(0)  # [1, 768]\n",
    "        # positive embeddings\n",
    "        positive = embeddings[i, 1].unsqueeze(0)  # [1, 768]\n",
    "        # 除了anchor和positive之外的所有embeddings作为负样本\n",
    "        negatives = embeddings[i, 2:]  # [13, 768]\n",
    "        # 计算当前batch的InfoNCE损失\n",
    "        loss = infoNCELoss(anchor, positive, negatives)\n",
    "        losses.append(loss)\n",
    "    # 计算整个batch的平均损失\n",
    "    batch_loss = torch.mean(torch.stack(losses))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6431)\n"
     ]
    }
   ],
   "source": [
    "# 假设 embeddings 是一个形状为 [batch, group_size, embedding_len] 的张量\n",
    "embeddings = torch.randn(2, 15, 768)  # 示例数据\n",
    "print(batchloss(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去掉模型的分类头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaModel(\n",
      "  (embeddings): XLMRobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): XLMRobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x XLMRobertaLayer(\n",
      "        (attention): XLMRobertaAttention(\n",
      "          (self): XLMRobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): XLMRobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): XLMRobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): XLMRobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): XLMRobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification, AutoModel, AutoTokenizer\n",
    "import torch\n",
    "model = AutoModel.from_pretrained('/mnt/bn/data-tns-live-llm/leon/experiments/llm/fcbank/xlmr/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/', torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/experiments/llm/fcbank/xlmr/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/\")\n",
    "# print(type(model.modules()))\n",
    "print(model)\n",
    "# print(model.roberta)\n",
    "# print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaForSequenceClassification(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "del model.classifier\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
